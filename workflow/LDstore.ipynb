{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9fbaa9f-c68d-4553-b6a1-6e9d4b040316",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# LDstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c2558e-4481-474d-8723-9d76ac952922",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Check tutorial [here](http://www.christianbenner.com/#)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e07551-3a85-4085-9251-ea5c0d11f446",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Installation\n",
    "\n",
    "```\n",
    "pip3 install https://files.pythonhosted.org/packages/a8/fd/f98ab7dea176f42cb61b80450b795ef19b329e8eb715b87b0d13c2a0854d/ldstore-0.1.9.tar.gz \n",
    "\n",
    "pip install pyliblzma\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7e493a-c99c-4a71-8894-b31658163f67",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Create master file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ede3fb5-d54b-4ef9-9088-489933fb1bce",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The master file is a semicolon-separated text file and contains no space. It contains the following mandatory column names and one dataset per line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a38f88a-4d5f-4b50-9535-4bf46ce1f31c",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "For the Z file modify this file to be rsid:chrom:pos:a1:a2. Formatting for chromosome should be 01,02,03...etc\n",
    "\n",
    "For the sample files remember to use only unrelated individuals from the 500K genotyped participants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a2abcd-dd1a-4663-99b7-f9b6978c60a4",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Minimal working example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ad078a-7120-4c87-a91e-95085801e501",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "### Step 1. Create the bgen_list_file\n",
    "\n",
    "This file contains 2 columns: path to bgen file; path to sample file\n",
    "\n",
    "The code I used to get the info for the UKB specific data was\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "bgen=glob.glob('/mnt/vast/hpc/csg/UKBiobank/results/pleiotropy_AD_ARHI/111822_LDstore_files/regions_chr1_22/'+'/01/*.bgen')\n",
    "df = pd.DataFrame({'bgen':bgen})\n",
    "df['sample'] = df.apply(lambda x:'/mnt/vast/hpc/csg/UKBiobank_Yale_transfer/ukb39554_imputeddataset/ukb32285_imputedindiv.sample', axis=1)\n",
    "df.to_csv('/mnt/vast/hpc/csg/UKBiobank/results/pleiotropy_AD_ARHI/111822_LDstore_files/regions_chr1_22/01/chr1_bgen_list.txt',sep=';', header=True, index=False)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb88e4-2ec2-4c3e-b415-68562a881b91",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "### Step 2. File formatting and subsetting in PLINK\n",
    "\n",
    "```\n",
    "sos run LDstore.ipynb \\\n",
    "    subset_bgen \\\n",
    "    --cwd test \\\n",
    "    --masterfile masterfile_name \\\n",
    "    --bgen_list_file test/data/chr1_bgen_list.txt \\\n",
    "    --maf_filter 0.001 \\\n",
    "    --numThreads 10 \\\n",
    "    --mem 10G \\\n",
    "    --job_size 1 \\\n",
    "    --container ~/containers/lmm.sif\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b417487e-bf34-44aa-a631-664dad183b15",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "### Step 3. Create the z (variant list) file to run LDstore based on snplist file generated in previous step\n",
    "\n",
    "```\n",
    "sos run LDstore.ipynb \\\n",
    "    z_file \\\n",
    "    --cwd test \\\n",
    "    --masterfile \\\n",
    "    --numThreads 10 \\\n",
    "    --mem 10G \\\n",
    "    --job_size 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09273da6-f28c-41ec-b4fa-11f3f7be5960",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "### Step 4. Create the new masterfile based on subset bgen files and newly generated z files\n",
    "\n",
    "\n",
    "In this step you need to provide a string for the masterfile name. The samples to include format can be look at [LDstore tutorial](http://www.christianbenner.com/#)\n",
    "\n",
    "```\n",
    "sos run ~/project/UKBB_GWAS_dev/workflow/111722_LDstore.ipynb \\\n",
    "    masterfile \\\n",
    "    --cwd test \\\n",
    "    --masterfile masterfile_name\\\n",
    "    --number_of_samples 351430 \\\n",
    "    --incl_samples test/samples_to_include.incl \\\n",
    "    --numThreads 10 \\\n",
    "    --mem 1G \\\n",
    "    --job_size 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bf786d-9981-4db8-ae27-1bb8651cf6fd",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "### Step 5. Get the bcor (variant correlation) files using LDstore\n",
    "```\n",
    "sos run LDstore.ipynb \\\n",
    "    bcor\\\n",
    "    --cwd test \\\n",
    "    --masterfile masterfile_name\\\n",
    "    --numThreads 10 \\\n",
    "    --mem 100G \\\n",
    "    --job_size 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb62d667-0d11-43cc-b4d4-aae3c021d5f2",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Step 6. Generate ld text files and save output with xz compression format\n",
    "\n",
    "```\n",
    "sos run LDstore.ipynb \\\n",
    "    ld\\\n",
    "    --cwd test \\\n",
    "    --masterfile masterfile_name\\\n",
    "    --numThreads 10 \\\n",
    "    --mem 100G \\\n",
    "    --job_size 1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab9c6c3-aebd-4dea-95d1-3470f7dea6c6",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Step 7. Optional - LiftOver the coordinates from hg19 to hg38 usign the bim file\n",
    "\n",
    "This step is accomplished using some awk code in combination to [UCSC liftOver](https://genome.ucsc.edu/FAQ/FAQdownloads.html#liftOver) command line tool. \n",
    "\n",
    "```\n",
    "sos run LDstore.ipynb \\\n",
    "     liftover \\\n",
    "    --cwd test \\\n",
    "    --masterfile 'masterfile_name'\\\n",
    "    --bim_name `echo test/*_hg19.bim` \\\n",
    "    --to_build hg38 \\\n",
    "    --chain_file test/hg19ToHg38.over.chain.gz\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff109cb-404f-49ae-87bd-8934fed65cfa",
   "metadata": {
    "kernel": "Python3"
   },
   "source": [
    "# Run LDstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8517236-02b7-4e6d-ae2b-951905b42bb5",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Working directory: change accordingly\n",
    "parameter: cwd = path\n",
    "# Path to bgen or plink files\n",
    "parameter: masterfile = ''\n",
    "# Memory allocated\n",
    "parameter: mem = '80G'\n",
    "## Walltime for the job\n",
    "parameter: walltime = '36h'\n",
    "# Number of Threads\n",
    "parameter: numThreads = 4\n",
    "# Job size\n",
    "parameter: job_size = 1\n",
    "# The container with the specific software\n",
    "parameter: container = 'statisticalgenetics/lmm:2.4'\n",
    "parameter: container_liftover = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbeac9a-bbd1-42c7-8b33-f10256c717a4",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Step 2. Subset bgen files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caef86a-4b77-428c-83f7-9eedd3e8f5f4",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Subset bgen files to a specific maf, change the variant id, write snplist to create *z file and output bim files for downstream liftover\n",
    "[subset_bgen]\n",
    "import pandas as pd\n",
    "parameter: bgen_list_file = path\n",
    "bgen_list = pd.read_csv(bgen_list_file,sep=\";\")\n",
    "# This creates a list in which [0] is the original *.bgen file and [1] is the *.sample file \n",
    "input_list = bgen_list.iloc[:,[0,1]].values.tolist()\n",
    "parameter: maf_filter = 0.001\n",
    "input:input_list, group_by = 2\n",
    "output: f'{cwd:a}/{_input[0]:bn}.{maf_filter}.subset.bgen', \n",
    "        f'{cwd:a}/{_input[0]:bn}.{maf_filter}.subset.sample', \n",
    "        f'{cwd:a}/{_input[0]:bn}.{maf_filter}.subset.snplist',\n",
    "        f'{cwd:a}/{_input[0]:bn}.{maf_filter}.subset.bim',\n",
    "        f'{cwd:a}/{_input[0]:bn}.{maf_filter}.subset.bgen.bgi'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '48h', mem = mem, cores = numThreads, tags = f'{_input[0]:bn}'\n",
    "bash: container = container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    plink2 --bgen ${_input[0]} 'ref-first' \\\n",
    "    --sample ${_input[1]} \\\n",
    "    --write-snplist \\\n",
    "    --out ${_output[0]:n} \\\n",
    "    --maf ${maf_filter} \\\n",
    "    --export bgen-1.2 'bits=8' 'ref-first' \\\n",
    "    --set-all-var-ids '@:#:$r:$a' \\\n",
    "    --new-id-max-allele-len 100 \\\n",
    "    --make-just-bim\n",
    "\n",
    "    bgenix -g ${_output[0]} -index\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76bd453-9e5e-453a-b026-8ffe8aeae4c9",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Step 3. Create the z file using the snplist from previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843ffdc8-ecf9-4c4e-8027-7e722734ce34",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[z_file (Creation of the variant list for LDStore2)]\n",
    "import glob\n",
    "snplist_files = glob.glob(f'{cwd}/*.snplist', recursive=True)\n",
    "input: snplist_files, group_by = 1\n",
    "output: f'{cwd:a}/{_input:bn}.z'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '48h', mem = mem, cores = numThreads, tags = f'{_input:bn}'\n",
    "python: expand = \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout'\n",
    "    import pandas as pd\n",
    "    snplist = pd.read_csv('${_input}', header=None, sep='\\t', names=[\"rsid\"])\n",
    "    snplist[['chromosome', 'position', 'allele1', 'allele2']] = snplist['rsid'].str.split(':', expand=True)\n",
    "    snplist[[\"rsid\",\"chromosome\", \"position\", \"allele1\", \"allele2\"]].to_csv('${_output}', header=True, index=False, sep= ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45ecafa-ee94-44d8-85ce-f4841b93723c",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Step 4. Create the new masterfile with subsetted bgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8ebbc1-f391-4c55-adfb-8679f21f8440",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Creation of the masterfile\n",
    "[masterfile (Creation of the masterfile): provides=f'{cwd}/{masterfile}.masterfile']\n",
    "# Number of samples present in the bgen file\n",
    "parameter: number_of_samples=int\n",
    "# Parameter for samples to be included the name has to end in .incl\n",
    "parameter: incl_samples = path('.')\n",
    "output: f'{cwd}/{masterfile}.masterfile'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "python: expand = \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout'\n",
    "\n",
    "    import glob\n",
    "    import pandas as pd\n",
    "\n",
    "    bgen = glob.glob(${cwd:r}+'/*.bgen')\n",
    "    masterfile = pd.DataFrame({\n",
    "        \"z\": glob.glob(${cwd:r}+'/*.z'),\n",
    "        \"bgen\": glob.glob(${cwd:r}+'/*.bgen'),\n",
    "        \"bgi\": [i.replace('bgen', 'bgen.bgi') for i in bgen],\n",
    "        \"bcor\": [i.replace('bgen', 'bcor') for i in bgen],\n",
    "        \"ld\": [i.replace('bgen', 'ld') for i in bgen],\n",
    "        \"sample\": [i.replace('bgen', 'sample') for i in bgen],\n",
    "    })\n",
    "\n",
    "    #Add a constant number to every row in the sample column (the number of samples to analyze)\n",
    "    masterfile['n_samples'] =  masterfile.apply(lambda x:${number_of_samples}, axis=1)\n",
    "    #Add a constant number to every row in the sample column (the samples to include in the analysis)\n",
    "    masterfile['incl'] = masterfile.apply(lambda x:${incl_samples:r}, axis=1)\n",
    "    masterfile = masterfile[['z', 'bgen', 'bgi', 'bcor', 'ld', 'n_samples', 'sample', 'incl']]\n",
    "    masterfile.to_csv(${_output:r}, sep=\";\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfface8-0cf6-421b-adaf-8d0657bb1e86",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Step 5 and 6 Running LDStore for BCOR and LD file creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828a624d-c46a-4225-83d6-f37c46fa5a70",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create bdose file\n",
    "[bdose]\n",
    "input: f'{cwd}/{masterfile}.masterfile'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '48h', mem = mem, cores = numThreads, tags = f'{_input:bn}'\n",
    "bash: expand= \"${ }\", stderr = f'{_input}.{step_name}.stderr', stdout = f'{_input}.{step_name}.stdout'   \n",
    "    ~/ldstore_v2.0_x86_64/./ldstore_v2.0_x86_64  \\\n",
    "    --in-files ${_input:n} \\\n",
    "    --write-bcor --write-bdose --bdose-version 1.1 \\\n",
    "    --n-threads ${numThreads} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cfecad-c8c0-416e-88c3-7c3aba9fa6be",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Create bcor files\n",
    "[bcor]\n",
    "#FIXME track the generation of the bcor files\n",
    "import pandas as pd\n",
    "master_list = pd.read_csv(f'{cwd}/{masterfile}.masterfile',sep=\";\")\n",
    "input_list = master_list.iloc[:,[1]].values.tolist()\n",
    "input: input_list, group_by=1\n",
    "output: f'{cwd}/{_input:bn}.bcor'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '48h', mem = mem, cores = numThreads, tags = f'{_input:bn}'\n",
    "bash: expand= \"${ }\", stderr = f'{_input}.{step_name}.stderr', stdout = f'{_input}.{step_name}.stdout'   \n",
    "    ~/ldstore_v2.0_x86_64/./ldstore_v2.0_x86_64  \\\n",
    "    --in-files f'${cwd}/${masterfile}.masterfile'\\\n",
    "    --write-bcor \\\n",
    "    --read-only-bgen \\\n",
    "    --n-threads ${numThreads} \\\n",
    "    --compression 'high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281e57c0-e2cf-4c73-aabc-9df7dd4a61f0",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Calculate LD\n",
    "[ld_1]\n",
    "import pandas as pd\n",
    "master_list = pd.read_csv(f'{cwd}/{masterfile}.masterfile',sep=\";\")\n",
    "input_list = master_list.iloc[:,[3]].values.tolist()\n",
    "input: input_list, group_by=1\n",
    "output: f'{cwd}/{_input:bn}.ld'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '48h', mem = mem, cores = numThreads, tags = f'{_input:bn}'\n",
    "bash: expand= \"${ }\", stderr = f'{_input}.{step_name}.stderr', stdout = f'{_input}.{step_name}.stdout'   \n",
    "    ~/ldstore_v2.0_x86_64/./ldstore_v2.0_x86_64 \\ \\\n",
    "    --bcor-to-text \\\n",
    "    --bcor-file ${_input} \\\n",
    "    --ld-file ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaf1eff-400e-4d65-a7bb-0b868ff9ff08",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Output LD matriz as compressed xz format\n",
    "[ld_2]\n",
    "import pandas as pd\n",
    "master_list = pd.read_csv(f'{cwd}/{masterfile}.masterfile',sep=\";\")\n",
    "input_list = master_list.iloc[:,[0,4]].values.tolist()\n",
    "input:input_list, group_by = 2\n",
    "output:  f'{cwd:a}/{_input[0]:bn}.xz'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '48h', mem = mem, cores = numThreads, tags = f'{_input[0]:bn}'\n",
    "python: expand= \"${ }\", stderr = f'{_output}.{step_name}.stderr', stdout = f'{_output}.{step_name}.stdout'\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import xz\n",
    "\n",
    "    # This corresponds to the variant names (rsids) present in the zfile\n",
    "    # For liftover we will use the bim file generated in step 2 of this workflow\n",
    "    #z_file=pd.read_csv(${_input[0]:r}, sep=\" \", skiprows=1, header=None)[0].to_numpy()\n",
    "    np_ld = np.loadtxt(${_input[1]:r}, dtype = \"float16\")\n",
    "    np.set_printoptions(formatter={'float': lambda x: \"{0:0.6f}\".format(x)})\n",
    "    tri_lower_diag = np.tril(np_ld, k=0)\n",
    "    # To save lower triangle and diagonal ld matrix as xz file. Everything above the diagonal is 0.0\n",
    "    import xz\n",
    "    with xz.open('${_output}', \"w+\", preset=9) as f:\n",
    "        for r in range(tri_lower_diag.shape[0]):\n",
    "            f.write(\" \".join([\"{:.6f}\".format(x) for x in tri_lower_diag[r, :]]).encode())\n",
    "            f.write(b\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c9f424-eb82-4768-81fa-34fd1a2890af",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Run LiftOver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc45f34f-db57-473a-be6b-145c9cef61da",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Run liftover\n",
    "[liftover_1]\n",
    "# Genome build to which you like to liftover your data\n",
    "parameter: to_build = 'hg38'\n",
    "parameter: chain_file = path\n",
    "parameter: bim_name = paths\n",
    "input: bim_name, group_by=1\n",
    "output: f'{cwd:a}/{_input:bn}.bed',\n",
    "        f'{cwd:a}/{_input:bn}.{to_build}.bed',\n",
    "        f'{cwd:a}/{_input:bn}.{to_build}.unmapped.bed'\n",
    "        #f'{cwd:a}/{_input:bn}.{to_build}.bim'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '48h', mem = mem, cores = numThreads, tags = f'{_input:bn}'\n",
    "bash: container=container_liftover, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "\n",
    "    #First take the bim file generated in subset-bgen step and re-format to march bed file\n",
    "    awk '{print \"chr\"$1,$4,$4,$2,$3,$5,$6}' ${_input} > ${_output[0]}\n",
    "    #Now run liftover\n",
    "    /home/dmc2245/liftover_ucsc/liftOver ${_output[0]} ${chain_file} ${_output[1]} ${_output[2]}\n",
    "    # Count the unmapped positions\n",
    "    echo \"The number of unmapped variants is:\"\n",
    "    wc -l ${_output[2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32616f54-4628-44f5-8d30-95ec995a552a",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Organize bim file to account for unmapped variants\n",
    "[liftover_2]\n",
    "# Genome build to which you like to liftover your data\n",
    "parameter: to_build = 'hg38'\n",
    "parameter: bim_name = paths\n",
    "input: bim_name, group_by=1\n",
    "output: f'{cwd:a}/{_input:bn}.{to_build}.bim'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '48h', mem = mem, cores = numThreads, tags = f'{_input:bn}'\n",
    "python: container=container_liftover, expand= \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout'\n",
    "    import pandas as pd \n",
    "    def merge_dataframes(df1, df2):\n",
    "        # Merge DataFrames using a left join on the 'id' column\n",
    "        merged_df = pd.merge(df1, df2, on='id', how='left', suffixes=('_df1', '_df2'))\n",
    "        # Create a list to store dataframes\n",
    "        result_dfs = []\n",
    "        # Iterate through rows\n",
    "        for index, row in merged_df.iterrows():\n",
    "            if not pd.isna(row['pos_hg38']):  # If 'pos_hg38' is present in df2\n",
    "                id_string = f\"{int(row['chr_df2'])}:{int(row['pos_hg38'])}:{row['major_allele_df2']}:{row['minor_allele_df2']}\"\n",
    "                result_row = pd.Series([int(row['chr_df2']), id_string, int(row['cm_df2']), int(row['pos_hg38']), row['minor_allele_df2'], row['major_allele_df2']],\n",
    "                                       index=['chr_df2', 'id', 'cm_df2', 'pos_hg38', 'minor_allele_df2', 'major_allele_df2'])\n",
    "            else:\n",
    "                 result_row = pd.Series([0, row['id'], int(row['cm_df1']), int(row['pos_hg19']), row['minor_allele_df1'], row['major_allele_df1']],\n",
    "                                       index=['chr_df2', 'id', 'cm_df2', 'pos_hg38', 'minor_allele_df2', 'major_allele_df2'])\n",
    "\n",
    "            result_dfs.append(result_row)\n",
    "        # Concatenate the list of DataFrames into the final result DataFrame\n",
    "        # Order of results in DataFrame will match with df1\n",
    "        result_df = pd.DataFrame(result_dfs)\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    df1=pd.read_csv('${_input}',sep='\\t', header=None, names=[\"chr\",\"id\", \"cm\", \"pos_hg19\",\"minor_allele\",\"major_allele\"])\n",
    "    df2=pd.read_csv(f'${_input:n}.${to_build}.bed',sep='\\t', header=None, names=[\"chr\",\"pos_hg38\", \"end\", \"id\", \"cm\",\"minor_allele\",\"major_allele\"])\n",
    "    df2['chr'] = df2['chr'].str.replace('chr', '')\n",
    "    df_final= merge_dataframes(df1,df2)\n",
    "    df_final.to_csv('${_output}', header=False, sep=' ', index=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "calysto_bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "Python3",
     "python3",
     "Python3",
     "#FFD91A",
     {
      "name": "ipython",
      "version": 3
     }
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.24.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
